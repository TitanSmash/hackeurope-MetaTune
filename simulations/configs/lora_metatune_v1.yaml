experiment_id: lora_metatune_v1
seed: 42

format_policy: instruction_chat
split_policy: native_then_95_5
metafeature_sample_tokens: 2000000
objectives:
  - val_loss
  - gpu_hours

datasets:
  - id: roneneldan/TinyStories
    max_total_bytes: 209715200
  - id: HuggingFaceH4/ultrachat_200k
    max_total_bytes: 209715200
  - id: teknium/OpenHermes-2.5
    max_total_bytes: 209715200
  - id: Open-Orca/OpenOrca
    max_total_bytes: 209715200
  - id: meta-math/MetaMathQA
    max_total_bytes: 209715200

paths:
  raw_root: outputs/raw_datasets
  processed_root: outputs/datasets
  manifests_root: outputs/manifests
  history_root: outputs/history
  slurm_root: outputs/slurm
  hf_cache_dir: outputs/hf_cache

training:
  mode: simulate
  token_budget_per_trial: 25000000
  seq_len: 1024
  eval_points: 20
  gpu_count: 1
  lora:
    targets: attention_proj
  nanogpt:
    repo_dir: simulations/nanoGPT-LoRA-master
    dataset_prefix: metatune
  # For real NanoGPT-LoRA integration, switch mode to external and provide a command template.
  # external_command: >
  #   python -m simulations.nanogpt_adapter
  #   --repo-dir simulations/nanoGPT-LoRA-master
  #   --config-file config/lora_shakespeare.py
  #   --dataset-id "{dataset_id}" --train-path "{train_path}" --val-path "{val_path}"
  #   --run-id "{run_id}" --token-budget {token_budget} --seq-len {seq_len}
  #   --hp-json '{hp_json}' --metrics-path "{metrics_path}" --curve-path "{curve_path}"
  #   --device cuda --compile False --gradient-accumulation-steps 1

search:
  stage1_trials: 96
  stage2_trials: 48
  ingest_every: 12
  seed_policy: one_then_three_top
  seed_stage1: [1]
  seed_reruns: [2, 3]
  top_k_reruns: 12
  hp_space:
    lora_r:
      type: categorical
      values: [4, 8, 16, 32, 64]
    learning_rate:
      type: float
      low: 0.00005
      high: 0.003
      log: true
    lora_alpha:
      type: categorical
      values: [8, 16, 32, 64, 128]
    lora_dropout:
      type: float
      low: 0.0
      high: 0.2
      log: false
    batch_size:
      type: categorical
      values: [4, 8, 16]

retry:
  max_retries: 1



local:
  memory_guard: 0.90
  max_parallel: 32
  poll_sec: 5.0
